{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e431a3f3",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de67000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Max\\Desktop\\TREC2025\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from openai._exceptions import OpenAIError, RateLimitError\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import faiss\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from collections import Counter\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a346cd33",
   "metadata": {},
   "source": [
    "Create dataFrame for the question assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a168c31c4f5fa45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:21:22.267037Z",
     "start_time": "2025-05-28T13:21:22.261377Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Assessment</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clueweb22-en0030-87-05450</td>\n",
       "      <td>1</td>\n",
       "      <td>Who is Helen Russell, the author of the articl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clueweb22-en0030-87-05450</td>\n",
       "      <td>1</td>\n",
       "      <td>Has the show John Dillermand received support ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clueweb22-en0030-87-05450</td>\n",
       "      <td>1</td>\n",
       "      <td>What is the professional credentials of Christ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clueweb22-en0030-87-05450</td>\n",
       "      <td>1</td>\n",
       "      <td>Who is Erla Heinesen Højsted, cited in the art...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clueweb22-en0030-87-05450</td>\n",
       "      <td>1</td>\n",
       "      <td>Does The Guardian clearly described how the sh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Article Assessment  \\\n",
       "0  clueweb22-en0030-87-05450          1   \n",
       "1  clueweb22-en0030-87-05450          1   \n",
       "2  clueweb22-en0030-87-05450          1   \n",
       "3  clueweb22-en0030-87-05450          1   \n",
       "4  clueweb22-en0030-87-05450          1   \n",
       "\n",
       "                                            Question  \n",
       "0  Who is Helen Russell, the author of the articl...  \n",
       "1  Has the show John Dillermand received support ...  \n",
       "2  What is the professional credentials of Christ...  \n",
       "3  Who is Erla Heinesen Højsted, cited in the art...  \n",
       "4  Does The Guardian clearly described how the sh...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the path to the text file\n",
    "file_path = '2024-question-assessment.txt'\n",
    "\n",
    "array = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.split(\"\\t\")\n",
    "        if (line[5] == \"4\" or line[5] == \"3\" or line[5] == \"1\"):\n",
    "            array.append(line)\n",
    "            \n",
    "dataFrame = pd.DataFrame(array)\n",
    "dataFrame.columns = [\"AID\", \"Article\", \"RID\", \"Model\", \"Order\", \"Assessment\", \"Useless\", \"Question\"]\n",
    "dataFrame.drop(columns=[\"AID\", \"Model\", \"RID\", \"Useless\", \"Order\"], inplace=True)\n",
    "\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d47ae8",
   "metadata": {},
   "source": [
    "Prompt for labeling our data into categories, outputs as an csv file named output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b85cc2af186e4b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  47%|████▋     | 53/113 [02:51<04:12,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 53 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  85%|████████▍ | 96/113 [05:44<01:03,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 95 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  89%|████████▉ | 101/113 [06:04<00:47,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 113/113 [06:49<00:00,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total processed: 5698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Adjust with your data loading method\n",
    "qArray = [row[7] for row in array]\n",
    "\n",
    "categories = [\n",
    "    \"Author Bias\",\n",
    "    \"Claim Verification\",\n",
    "    \"Source Background & Credibility\",\n",
    "    \"Publication Reputation\",\n",
    "    \"Public Reaction\",\n",
    "    \"Bad Question\"\n",
    "]\n",
    "\n",
    "def format_prompt(qs):\n",
    "    header = (\n",
    "        f\"Classify each of the following questions into one of these categories per line: {', '.join(categories)}.\\n\\n\"\n",
    "        \"These are the criteria for a bad question:\\n\"\n",
    "        \"- The question assumes prior knowledge of the article or refers vaguely to \\\"the claim,\\\" \\\"the event,\\\" or \\\"this person\\\" without explanation.\\n\"\n",
    "        \"- If the question contains \\\"in the article\\\" it is a bad question.\\n\"\n",
    "        \"- The question can’t be answered from a single source or needs synthesis from multiple unrelated documents.\\n\"\n",
    "        \"- The question contains multiple parts, asks more than one thing at once, or is vague in its focus.\\n\"\n",
    "        \"- The question asks for interpretations, motivations, or hypothetical outcomes instead of concrete, verifiable facts.\"\n",
    "        '\\nReturn the result as plain CSV with no extra text.'\n",
    "        ' Format like this:\\n\\n'\n",
    "        '\"Example question here?\",\"Author Bias\"\\n'\n",
    "    )\n",
    "    body = \"\\n\".join([f\"\\\"{q.strip()}\\\"\" for q in qs])\n",
    "    return header + body\n",
    "\n",
    "def process_batch(index, chunk, retries=3):\n",
    "    prompt = format_prompt(chunk)\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "                max_tokens=4096,\n",
    "            )\n",
    "            return index, response.choices[0].message.content.strip().splitlines()\n",
    "        except (OpenAIError, RateLimitError) as e:\n",
    "            wait = 2 ** attempt\n",
    "            time.sleep(wait)\n",
    "    return index, []\n",
    "\n",
    "batch_size = 50\n",
    "batches = [(i, qArray[i:i + batch_size]) for i in range(0, len(qArray), batch_size)]\n",
    "\n",
    "results = {}\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(process_batch, i, chunk) for i, chunk in batches]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing\"):\n",
    "        idx, lines = future.result()\n",
    "        if lines:\n",
    "            results[idx] = lines\n",
    "        else:\n",
    "            print(f\"Batch {idx // batch_size} failed.\")\n",
    "\n",
    "# Sort results by original batch order\n",
    "ordered = []\n",
    "for i in sorted(results):\n",
    "    ordered.extend(results[i])\n",
    "\n",
    "print(f\"✅ Total processed: {len(ordered)}\")\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"output.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write('\"question\",\"category\"\\n')\n",
    "    for line in ordered:\n",
    "        if \",\" in line:\n",
    "            f.write(line.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629aa682",
   "metadata": {},
   "source": [
    "Split Data into train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06022dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(Question, Category):\n",
    "    Q_train, Q_test, c_train, c_test = train_test_split(Question, Category, test_size=0.3, random_state=42)\n",
    "    return Q_train, Q_test, c_train, c_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dfa74e",
   "metadata": {},
   "source": [
    "Load all data into train_test_split and add it to a BERT readable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1f3670f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"output.csv\", header=None, names=[\"Question\", \"Category\"])\n",
    "\n",
    "col1 = df1.iloc[:, 0].to_numpy()\n",
    "col2 = df1.iloc[:, 1].to_numpy()\n",
    "\n",
    "Q_train, Q_test, c_train, c_test = splitData(col1, col2)\n",
    "\n",
    "# Load tokenizer and model with built-in classification head\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, questions, categories):\n",
    "        self.encodings = tokenizer(questions, truncation=True, padding=True)\n",
    "        self.labels = torch.tensor(le.fit_transform(categories))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = TextDataset(list(Q_train), list(c_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2965aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count label frequencies\n",
    "label_counts = Counter([dataset[i][\"labels\"].item() for i in range(len(dataset))])\n",
    "\n",
    "# Make sure the order matches label encoding\n",
    "num_labels = len(label_counts)\n",
    "total = sum(label_counts.values())\n",
    "weights = [total / (num_labels * label_counts[i]) for i in range(num_labels)]\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb6ac1",
   "metadata": {},
   "source": [
    "Defines hyperparameters for the model and trains it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc98ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=3,\n",
    "    auto_find_batch_size = True,\n",
    "    save_strategy=\"no\",\n",
    "    output_dir=\"fine_tuned_model\"\n",
    ")\n",
    "\n",
    "# Set up Trainer API to handle training loop\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b393eb9",
   "metadata": {},
   "source": [
    "Train BERT using KFOLD validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b84ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df1 = pd.read_csv(\"output.csv\", header=None, names=[\"Question\", \"Category\"])\n",
    "col1 = df1.iloc[:, 0].to_numpy()\n",
    "col2 = df1.iloc[:, 1].to_numpy()\n",
    "# Initialize label encoder and tokenizer\n",
    "le = LabelEncoder()\n",
    "le.fit(col2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Change to your model if needed\n",
    "# Custom Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(le.transform(labels))\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "# Cross-validation setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "for fold, (train_indices, val_indices) in enumerate(skf.split(col1, col2)):\n",
    "    print(f\"\\nTraining Fold {fold+1}/5\")\n",
    "    # Convert from NumPy array to list[str]\n",
    "    train_texts = col1[train_indices].tolist()\n",
    "    train_labels = col2[train_indices].tolist()\n",
    "    val_texts = col1[val_indices].tolist()\n",
    "    val_labels = col2[val_indices].tolist()\n",
    "    # Prepare datasets\n",
    "    train_dataset = TextDataset(train_texts, train_labels)\n",
    "    val_dataset = TextDataset(val_texts, val_labels)\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    # Reinitialize model per fold\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(le.classes_))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(3):  # Adjust as needed\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_labels_encoded = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.logits, dim=1)\n",
    "            val_predictions.extend(predicted.cpu().numpy())\n",
    "            val_labels_encoded.extend(labels.cpu().numpy())\n",
    "    # Decode labels\n",
    "    predicted_labels = le.inverse_transform(val_predictions)\n",
    "    real_labels = le.inverse_transform(val_labels_encoded)\n",
    "    # Report accuracy\n",
    "    accuracy = accuracy_score(val_labels_encoded, val_predictions)\n",
    "    fold_accuracies.append(accuracy)\n",
    "    print(f\"Fold {fold+1} Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(real_labels, predicted_labels))\n",
    "# Average score\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f\"\\nAverage Cross-Validation Accuracy: {average_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6b9caf",
   "metadata": {},
   "source": [
    "Load trained model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b75d1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d013eae",
   "metadata": {},
   "source": [
    "Function for making predictions with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a1a29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Test_Questions: list, batch_size: int):\n",
    "    predictions = []\n",
    "    for i in tqdm(range(0, len(Test_Questions), batch_size), desc=\"Running inference\"):\n",
    "        batch = Test_Questions[i:i+batch_size]\n",
    "        test_encodings = tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**test_encodings)\n",
    "            batch_preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(batch_preds)\n",
    "\n",
    "    # Convert predicted tensor to list of integers\n",
    "    predicted_labels = [label.item() for label in predictions]\n",
    "\n",
    "    predicted_labels = le.inverse_transform(predicted_labels)\n",
    "\n",
    "    # Print results\n",
    "    for i in range(len(Test_Questions)):\n",
    "        print(Test_Questions[i] + \"----->\" + predicted_labels[i])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c87229e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_report(predictions: list, actual_labels: list):\n",
    "    # Convert predicted tensor to list of integers\n",
    "    predicted_labels = [label.item() for label in predictions]\n",
    "\n",
    "    predicted_labels = le.inverse_transform(predicted_labels)\n",
    "    # Accuracy\n",
    "    print(\"Accuracy:\", accuracy_score(actual_labels, predicted_labels))\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(actual_labels, predicted_labels))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=le.classes_,\n",
    "                yticklabels=le.classes_)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06161580",
   "metadata": {},
   "source": [
    "Prints out a classifcation report for the test data output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831bd9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|██████████| 1/1 [00:00<00:00, 23.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the sky blue?----->Claim Verification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Q_test_list = Q_test.astype(str).tolist()\n",
    "predictions = predict(Q_test_list, 64)\n",
    "\n",
    "c_test_list = c_test.astype(str).tolist()\n",
    "data_report(predictions, c_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2812b7",
   "metadata": {},
   "source": [
    "Creates a Pandas dataFrame from the clueweb database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a697066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38131\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ClueWeb22-ID</th>\n",
       "      <th>Clean-Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.dailymail.co.uk/news/article-10130...</td>\n",
       "      <td>clueweb22-en0032-91-05114</td>\n",
       "      <td>Japan's Princess Mako marries commoner, loses ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.nytimes.com/2021/08/26/opinion/afg...</td>\n",
       "      <td>clueweb22-en0027-70-17775</td>\n",
       "      <td>Opinion | Let’s Not Pretend That the Way We Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.politicshome.com/thehouse/article/...</td>\n",
       "      <td>clueweb22-en0032-18-01494</td>\n",
       "      <td>No longer can China’s atrocities against the U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://medicaladvise.org/clinical-trials-rese...</td>\n",
       "      <td>clueweb22-en0012-53-13803</td>\n",
       "      <td>How does molnupiravir work? - Medical Advise\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.euronews.com/2021/12/10/mexico-tru...</td>\n",
       "      <td>clueweb22-en0002-69-11564</td>\n",
       "      <td>Mexico truck crash: Dozens killed after lorry ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://www.dailymail.co.uk/news/article-10130...   \n",
       "1  https://www.nytimes.com/2021/08/26/opinion/afg...   \n",
       "2  https://www.politicshome.com/thehouse/article/...   \n",
       "3  https://medicaladvise.org/clinical-trials-rese...   \n",
       "4  https://www.euronews.com/2021/12/10/mexico-tru...   \n",
       "\n",
       "                ClueWeb22-ID  \\\n",
       "0  clueweb22-en0032-91-05114   \n",
       "1  clueweb22-en0027-70-17775   \n",
       "2  clueweb22-en0032-18-01494   \n",
       "3  clueweb22-en0012-53-13803   \n",
       "4  clueweb22-en0002-69-11564   \n",
       "\n",
       "                                          Clean-Text  \n",
       "0  Japan's Princess Mako marries commoner, loses ...  \n",
       "1  Opinion | Let’s Not Pretend That the Way We Wi...  \n",
       "2  No longer can China’s atrocities against the U...  \n",
       "3  How does molnupiravir work? - Medical Advise\\n...  \n",
       "4  Mexico truck crash: Dozens killed after lorry ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the path to the text file\n",
    "file_path = 'trec-2024-lateral-reading-task2-baseline-documents.jsonl'\n",
    "\n",
    "\n",
    "with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "clue_df = pd.DataFrame(data)\n",
    "\n",
    "print(len(clue_df))\n",
    "clue_df.drop(columns=[\"URL-hash\", \"Language\"], inplace=True)\n",
    "clue_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149dd51d",
   "metadata": {},
   "source": [
    "Create FAISS index and save it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68317718",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Good balance of speed/accuracy\n",
    "\n",
    "# Your text database\n",
    "texts = clue_df[\"Clean-Text\"].tolist()\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = model.encode(texts,batch_size=32, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Use IndexFlatIP with normalized vectors for cosine similarity\n",
    "\n",
    "np.save(\"embedding_matrix.npy\", embeddings)\n",
    "# Add embeddings to index\n",
    "index.add(embeddings)\n",
    "faiss.write_index(index, \"semantic_index.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a12f490",
   "metadata": {},
   "source": [
    "Load FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aebebe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(\"semantic_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc559480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'URL': 'https://poorrichardsnews.com/ana-navarro-is-a-moron/\\n', 'URL-hash': 'D06D4FDF2BF042D631B9C28D2A57C4D8', 'Language': 'en', 'ClueWeb22-ID': 'clueweb22-en0000-61-05623', 'Clean-Text': 'Ana Navarro is a moron – PoorRichardsNews.com\\nAna Navarro is a moron\\nHome /Illegal Aliens, International/Ana Navarro is a moron\\nIllegal Aliens,International|\\nOctober 23, 2019\\nNavarro denies Venezuela’s Maduro is a socialist\\nOn the view Ana Navarro cannot control her mouth.\\nThe View remains such an awful example example of how moronic Hollywood and commentators are who don’t know the facts.\\n“Maduro is NOT a socialist, he is a corrupt Murderer… blah, blah blah….”\\nAnna Navarro files her nails on TV\\nNavarro files her nails during border wall debate\\nWatch on\\n“Hypocrisy needs to be called out in American Politics, and the absurd has reached the point that it is insufferable”\\n“Democrat Congressman Ted Lieu and CNN’s resident RINO Ana Navarro both fell for a hoax tweet posted by leftist Time magazine editor-at-large Ian Bremmer. Why? Because they’re so desperate to push any anti-Trump narrative that they never bother to verify the information before screeching about it to the world.”\\nbizpacreview. com/2019/05/27/ted-lieu-and-ana-navarro-share-fake-trump-quote-pushed-by-time-mag-editor-were-waiting-for-apology-758854'}\n"
     ]
    }
   ],
   "source": [
    "vector = index.reconstruct(20)\n",
    "query = \"Newsweek Gets DESTROYED For Fearmongering on Kids & Vaccine\"\n",
    "query_embedding = model.encode(query, convert_to_numpy=True, normalize_embeddings=True).astype('float32')\n",
    "query_embedding = np.expand_dims(query_embedding, axis=0)\n",
    "scores, indices = index.search(query_embedding, 5)\n",
    "\n",
    "count = 0\n",
    "\n",
    "with open(\"trec-2024-lateral-reading-task2-baseline-documents.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if count == 19000:\n",
    "            data = json.loads(line)\n",
    "            print(data)  # do whatever with one object at a time\n",
    "            break\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c442c2",
   "metadata": {},
   "source": [
    "Query the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6e6f56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1, Score: 0.7362, ID: clueweb22-en0016-63-04220, Snippet: Super Glue | Bloons Wiki | Fandom Bloons Wiki 4,424 pages Explore Bloons Games Game Mechanics Other Pages Wiki-Related Info in: Glue Gunner, Bloons TD 6, Upgrades, and 5 more Super Glue Edit BTD6 BTDB...\n",
      "Rank: 2, Score: 0.4847, ID: clueweb22-en0022-56-01572, Snippet: Shattering Shells | Bloons Wiki | Fandom Bloons Wiki 4,428 pages Explore Bloons Games Game Mechanics Other Pages Wiki-Related Info in: Articles in construction with a WIP, Mortar Monkey, Bloons TD 6, ...\n",
      "Rank: 3, Score: 0.4604, ID: clueweb22-en0002-59-07035, Snippet: Sticky Bomb | Bloons Wiki | Fandom Bloons Wiki 4,428 pages Explore Bloons Games Game Mechanics Other Pages Wiki-Related Info in: Ninja Monkey, Bloons TD 6, Upgrades, and 4 more Sticky Bomb Edit BTD6 B...\n",
      "Rank: 4, Score: 0.4592, ID: clueweb22-en0011-30-13287, Snippet: Fortified Bloon | Bloons Wiki | Fandom Bloons Wiki 4,434 Explore Bloons Games Game Mechanics Other Pages Wiki-Related Info in: Bloons Pop!, Bloons, Bloons TD 6, and 3 more Fortified Bloon Edit Fortifi...\n",
      "Rank: 5, Score: 0.4528, ID: clueweb22-en0026-02-11527, Snippet: Blooncineration | Bloons Wiki | Fandom Bloons Wiki 4,435 pages Explore Bloons Games Game Mechanics Other Pages Wiki-Related Info in: Outdated pages, Mortar Monkey, Bloons TD 6, and 7 more Blooncinerat...\n"
     ]
    }
   ],
   "source": [
    "# Data prep\n",
    "texts = clue_df[\"Clean-Text\"].tolist()\n",
    "IDs = clue_df[\"ClueWeb22-ID\"].tolist()\n",
    "\n",
    "# Query\n",
    "query = \"How does the glue gunner work in BloonsTD6?\"\n",
    "query_tag = \"clueweb22-en0030-87-05450\"\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# Encode query\n",
    "query_embedding = model.encode(query, convert_to_numpy=True, normalize_embeddings=True).astype('float32')\n",
    "query_embedding = np.expand_dims(query_embedding, axis=0)  # shape (1, dim)\n",
    "\n",
    "# Load FAISS index (assumed to be cosine similarity = inner product of normalized vectors)\n",
    "index = faiss.read_index(\"semantic_index.faiss\")\n",
    "\n",
    "# Search\n",
    "k = 10  # search more than 5 in case we exclude the query itself\n",
    "scores, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Filter out the query itself and build results\n",
    "results = []\n",
    "for score, idx in zip(scores[0], indices[0]):\n",
    "    if IDs[idx] == query_tag:\n",
    "        continue  # skip the query origin\n",
    "\n",
    "    text = texts[idx]\n",
    "    snippet = text[:200].replace('\\n', ' ')\n",
    "    if len(text) > 200:\n",
    "        snippet += '...'\n",
    "\n",
    "    results.append({\n",
    "        \"rank\": len(results) + 1,\n",
    "        \"score\": round(float(score), 4),\n",
    "        \"snippet\": snippet,\n",
    "        \"id\": IDs[idx],\n",
    "    })\n",
    "\n",
    "    if len(results) == 5:\n",
    "        break  # stop once we have top 5 (excluding query)\n",
    "\n",
    "# Print results\n",
    "for r in results:\n",
    "    print(f\"Rank: {r['rank']}, Score: {r['score']}, ID: {r['id']}, Snippet: {r['snippet']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0504be2c",
   "metadata": {},
   "source": [
    "Load in marco database in batches into dataframe and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b536543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193732\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>docid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-60 Times - 0-60 | 0 to 60 Times &amp; 1/4 Mile T...</td>\n",
       "      <td>msmarco_v2.1_doc_00_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ethel Percy Andrus Gerontology Center [WorldCa...</td>\n",
       "      <td>msmarco_v2.1_doc_00_4810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Museo Nacional de Bellas Artes (Cuba) [WorldCa...</td>\n",
       "      <td>msmarco_v2.1_doc_00_18906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>File extension 00000 is used by operating syst...</td>\n",
       "      <td>msmarco_v2.1_doc_00_32860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Open 00001 File\\n\\nOpen 00001 File\\nTo open 00...</td>\n",
       "      <td>msmarco_v2.1_doc_00_37424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  0-60 Times - 0-60 | 0 to 60 Times & 1/4 Mile T...   \n",
       "1  Ethel Percy Andrus Gerontology Center [WorldCa...   \n",
       "2  Museo Nacional de Bellas Artes (Cuba) [WorldCa...   \n",
       "3  File extension 00000 is used by operating syst...   \n",
       "4  Open 00001 File\\n\\nOpen 00001 File\\nTo open 00...   \n",
       "\n",
       "                       docid  \n",
       "0      msmarco_v2.1_doc_00_0  \n",
       "1   msmarco_v2.1_doc_00_4810  \n",
       "2  msmarco_v2.1_doc_00_18906  \n",
       "3  msmarco_v2.1_doc_00_32860  \n",
       "4  msmarco_v2.1_doc_00_37424  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the path to the text file\n",
    "file_path = 'msmarco_v2.1_doc_00.json'\n",
    "\n",
    "\n",
    "with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "ms_df = pd.DataFrame(data)\n",
    "ms_df.drop(columns=[\"url\", \"title\", \"headings\"], inplace=True)\n",
    "print(len(ms_df))\n",
    "ms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6cbb25",
   "metadata": {},
   "source": [
    "Code for vectorizing marcoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_input_dir = \"\"\n",
    "output_dir = \"output/\"\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# Specify the path to the text file\n",
    "for entry in os.scandir(json_input_dir):\n",
    "    if entry.is_file():  # check if it's a file\n",
    "        print(\"Starting: \", entry.name)\n",
    "        with open(entry, 'r', encoding=\"utf8\") as f:\n",
    "            data = [json.loads(line) for line in f]\n",
    "        marco_df = pd.DataFrame(data)\n",
    "        marco_df.drop(columns=[\"url\", \"title\", \"headings\"], inplace=True)\n",
    "\n",
    "\n",
    "        # Your text database\n",
    "        texts = marco_df[\"body\"].tolist()\n",
    "\n",
    "        # Create embeddings\n",
    "        embeddings = model.encode(texts,batch_size=200, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "        # Initialize FAISS index\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatIP(dimension)  # Use IndexFlatIP with normalized vectors for cosine similarity\n",
    "\n",
    "        # Add embeddings to index\n",
    "        index.add(embeddings)\n",
    "        faiss.write_index(index, \"semantic_index_\" + entry.name[17] + entry.name[18] + \".faiss\")\n",
    "        data.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d08ba8",
   "metadata": {},
   "source": [
    "Simple Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "984dcbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 100%|██████████| 2000/2000 [1:31:47<00:00,  2.75s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved questions for 1444 articles to CSVs/article_questions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = 'trec-2024-lateral-reading-task2-baseline-documents.jsonl'\n",
    "with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "clueweb_df = pd.DataFrame(data)\n",
    "clueweb_df.drop(columns=[\"URL-hash\", \"Language\"], inplace=True)\n",
    "\n",
    "# Azure OpenAI setup\n",
    "endpoint = \"https://at23s-mb8dlh6z-eastus2.cognitiveservices.azure.com/\"\n",
    "model_name = \"gpt-4.1\"\n",
    "deployment = \"gpt-4.1\"\n",
    "subscription_key = \"650DCTqhZvPkvgkoUBIzKQogLDUINt8xfEnP0PIC0Y0mqWYryxBDJQQJ99BEACHYHv6XJ3w3AAAAACOGYEve\"\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    ")\n",
    "\n",
    "# Prompt setup\n",
    "prompt1 = (\n",
    "    \"You are a professional fact-checker and media literacy expert. Your task is to generate exactly 10 concise and actionable questions \"\n",
    "    \"that a thoughtful reader should ask when evaluating the trustworthiness of a news article.\\n\"\n",
    "    \"Each question should:\\n\"\n",
    "    \"- Be under 300 characters.\\n\"\n",
    "    \"- Address only one idea (no compound or multi-part questions).\\n\"\n",
    "    \"- Require external research (not answerable directly from the article).\\n\"\n",
    "    \"- Be specific to the article’s content.\\n\"\n",
    "    \"- Be clear and actionable, avoiding vagueness or academic jargon.\\n\"\n",
    "    \"Types of Questions to Include:\\n\"\n",
    "    \"1. Source History: e.g., What is [source name]'s history of reporting on this issue?\\n\"\n",
    "    \"2. Source Bias: e.g., What are the known biases of [source name]?\\n\"\n",
    "    \"3. Expert Credentials: e.g., What is [expert name]’s academic or professional background?\\n\"\n",
    "    \"4. Evidence Scrutiny: e.g., Are there independent studies supporting the claim about X?\\n\"\n",
    "    \"5. Perspective Balance: e.g., Are key opposing viewpoints on [issue] missing?\\n\"\n",
    "    \"6. Contextualization: e.g., Has this event occurred before, and how was it covered then?\\n\"\n",
    "    \"Examples of Ideal Output:\\n\"\n",
    "    \"- What is DR's history in producing children's television shows?\\n\"\n",
    "    \"- What is Erla Heinesen Højsted's professional background in children's psychology?\\n\"\n",
    "    \"- Is Christian Groes' view on patriarchal societies representative of the academic consensus?\\n\"\n",
    "    \"- Academic studies on using humor to teach children responsibility and accountability?\\n\"\n",
    "    \"- Male genitalia in kids' TV: aligned with international children's health recommendations?\\n\"\n",
    "    \"- Is filtering and diluting water enough to make it safe for ocean discharge?\\n\"\n",
    "    \"Final Instruction: Only return the 10 questions with no commentary. Each question must be under 300 characters, focused, \"\n",
    "    \"and answerable through research outside the article.\"\n",
    ")\n",
    "\n",
    "# Prepare data\n",
    "article_ids = clueweb_df[\"ClueWeb22-ID\"].tolist()\n",
    "article_texts = clueweb_df[\"Clean-Text\"].tolist()\n",
    "if len(article_ids) != len(article_texts):\n",
    "    raise ValueError(\"article_ids and article_texts must be the same length\")\n",
    "\n",
    "max_articles = 2000\n",
    "sample_indices = random.sample(range(len(article_ids)), min(len(article_ids), max_articles))\n",
    "output_csv_path = \"CSVs/article_questions.csv\"\n",
    "\n",
    "# Clean question utility\n",
    "def clean_question(text):\n",
    "    return re.sub(r\"^\\s*(\\d{1,2}[.)]|[-*])\\s*\", \"\", text).strip()\n",
    "\n",
    "max_article_chars = 30000  # Maximum characters per article\n",
    "\n",
    "# Worker function for a single article\n",
    "def process_article(idx):\n",
    "    article_id = article_ids[idx]\n",
    "    article_text = article_texts[idx]\n",
    "\n",
    "    if len(article_text) > max_article_chars:\n",
    "        return []  # Skip articles that are too long\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a professional fact-checker and media literacy expert.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt1}\\n\\nArticle:\\n{article_text}\\n\\nGenerate the 10 questions now.\"}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "        )\n",
    "\n",
    "        questions_text = response.choices[0].message.content.strip()\n",
    "        lines = [line.strip() for line in questions_text.split('\\n') if line.strip()]\n",
    "        question_list = [clean_question(line) for line in lines]\n",
    "        return [(article_id, q) for q in question_list[:10]]\n",
    "\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "\n",
    "# Run multithreaded processing\n",
    "results = []\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:  # Adjust `max_workers` as needed\n",
    "    futures = [executor.submit(process_article, idx) for idx in sample_indices]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing articles\"):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            results.extend(result)\n",
    "\n",
    "# Save to CSV\n",
    "with open(output_csv_path, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(f\"Saved questions for {len(results)//10} articles to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b57340",
   "metadata": {},
   "source": [
    "Bad Question Filtering and Question Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ddc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  13%|█▎        | 39/289 [02:01<16:54,  4.06s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 38 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|█▍        | 42/289 [02:09<12:01,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  19%|█▊        | 54/289 [03:03<17:45,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 54 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▍       | 71/289 [04:11<11:20,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 70 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  28%|██▊       | 81/289 [04:45<10:12,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 78 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  31%|███       | 90/289 [05:26<12:08,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 87 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  33%|███▎      | 94/289 [05:47<16:59,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 92 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  38%|███▊      | 109/289 [06:41<09:03,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 107 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  44%|████▎     | 126/289 [08:00<12:28,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 124 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  46%|████▌     | 132/289 [08:25<10:55,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 131 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|████▉     | 144/289 [09:12<09:10,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 142 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  52%|█████▏    | 149/289 [09:33<11:35,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 148 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  54%|█████▍    | 157/289 [09:58<08:11,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 154 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  56%|█████▌    | 161/289 [10:14<09:20,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 159 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 169/289 [10:44<07:48,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 167 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|█████▉    | 173/289 [10:59<08:09,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 171 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  61%|██████    | 176/289 [11:07<06:02,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 172 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  67%|██████▋   | 194/289 [12:27<08:01,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 191 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  72%|███████▏  | 207/289 [13:14<05:05,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 205 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  73%|███████▎  | 210/289 [13:27<05:07,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 207 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  76%|███████▌  | 220/289 [14:00<03:20,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 216 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  80%|███████▉  | 230/289 [14:43<03:11,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 228 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  82%|████████▏ | 238/289 [15:19<04:09,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 236 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  87%|████████▋ | 250/289 [16:12<03:14,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 249 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|█████████ | 261/289 [16:50<01:30,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 258 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  91%|█████████ | 262/289 [16:53<01:27,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 260 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  93%|█████████▎| 269/289 [17:20<01:11,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 267 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  95%|█████████▍| 274/289 [17:36<00:51,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 271 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  98%|█████████▊| 283/289 [18:10<00:20,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 279 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  99%|█████████▉| 286/289 [18:21<00:10,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 282 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 289/289 [18:26<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processed: 13232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   9%|▉         | 17/190 [01:20<11:39,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15 failed.\n",
      "Batch 16 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|█▍        | 28/190 [01:52<06:02,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 25 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|██        | 38/190 [02:37<08:49,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 35 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 43/190 [02:56<09:29,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 41 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  32%|███▏      | 60/190 [04:03<08:57,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 57 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|███▍      | 66/190 [04:27<08:35,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 65 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|███▌      | 67/190 [04:33<10:04,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 66 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  41%|████      | 77/190 [05:02<05:11,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 74 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  43%|████▎     | 82/190 [05:27<09:33,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 81 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  49%|████▉     | 93/190 [06:05<05:53,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 91 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 95/190 [06:11<04:59,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 92 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  54%|█████▍    | 103/190 [06:38<04:03,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 110/190 [07:09<05:19,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 109 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  62%|██████▏   | 118/190 [07:37<04:01,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 116 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  64%|██████▍   | 122/190 [07:53<04:42,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 120 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  71%|███████   | 134/190 [08:38<02:53,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 132 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  74%|███████▎  | 140/190 [09:09<05:10,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 139 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  81%|████████  | 153/190 [09:49<01:47,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 151 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  83%|████████▎ | 158/190 [10:09<02:08,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 156 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  87%|████████▋ | 166/190 [10:39<01:24,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 164 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  96%|█████████▌| 182/190 [11:45<00:30,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 181 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 190/190 [12:25<00:00,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processed: 8609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions = pd.read_csv(\"CSVs/article_questions.csv\", header=None)\n",
    "questions = questions[[1]].values.tolist()\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Adjust with your data loading method\n",
    "\n",
    "def format_prompt(qs):\n",
    "    header = (\n",
    "        f\"\"\"\n",
    "            You will review a list of questions and classify each one based on the rules below.\n",
    "\n",
    "            A question is a \"Bad Question\" if it violates **any** of the following:\n",
    "            - It assumes prior knowledge (\"the claim\", \"the author\", etc.) without clarity.\n",
    "            - It refers vaguely to the text or says \"in the article\" .\n",
    "            - It asks for multiple things at once (compound question).\n",
    "            - It is vague, broad, or asks for interpretation or hypotheticals.\n",
    "            - It cannot be answered using a single source/document.\n",
    "            - It includes multiple subjects or multiple objects.\n",
    "\n",
    "            If a question violates **any** of these rules, label it as \"Bad Question\".\n",
    "            Otherwise, label it as \"OK\".\n",
    "\n",
    "            Return plain CSV only. Format:\n",
    "            \"Question here\",\"Bad Question\" or \"OK\"\n",
    "\n",
    "            Example:\n",
    "            \"What is the author's intent and how do readers feel about it?\",\"Bad Question\"\n",
    "            \"What is the background of journalist Jane Doe?\",\"OK\"\n",
    "            \"\"\" \n",
    "        )\n",
    "\n",
    "    body = \"\\n\".join([f\"\\\"{q[0].strip()}\\\"\" for q in qs])\n",
    "    return header + body\n",
    "\n",
    "def process_batch(index, chunk, retries=3):\n",
    "    prompt = format_prompt(chunk)\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "                max_tokens=4096,\n",
    "            )\n",
    "            return index, response.choices[0].message.content.strip().splitlines()\n",
    "        except (OpenAIError, RateLimitError) as e:\n",
    "            wait = 2 ** attempt\n",
    "            time.sleep(wait)\n",
    "    return index, []\n",
    "\n",
    "batch_size = 50\n",
    "batches = [(i, questions[i:i + batch_size]) for i in range(0, len(questions), batch_size)]\n",
    "\n",
    "results = {}\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(process_batch, i, chunk) for i, chunk in batches]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing\"):\n",
    "        idx, lines = future.result()\n",
    "        if lines:\n",
    "            results[idx] = lines\n",
    "        else:\n",
    "            print(f\"Batch {idx // batch_size} failed.\")\n",
    "\n",
    "# Sort results by original batch order\n",
    "ordered = []\n",
    "for i in sorted(results):\n",
    "    ordered.extend(results[i])\n",
    "\n",
    "print(f\"Total processed: {len(ordered)}\")\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"CSVs/bad_questions.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    # Filter out code block markers if present\n",
    "    filtered_lines = [line for line in ordered if line.strip() not in (\"```\", \"```plaintext\")]\n",
    "\n",
    "    for line in filtered_lines:\n",
    "        line = line.strip().strip('\"')\n",
    "        if not line:\n",
    "            continue\n",
    "        # Split on the last comma\n",
    "        if \",\" in line:\n",
    "            split_idx = line.rfind(\",\")\n",
    "            question = line[:split_idx].strip().strip('\"')\n",
    "            category = line[split_idx + 1:].strip().strip('\"')\n",
    "            writer.writerow([question, category])\n",
    "\n",
    "with open(\"CSVs/bad_questions.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ok_rows = []\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if len(row) >= 2 and row[1].strip() == \"OK\":\n",
    "            ok_rows.append([row[0]])\n",
    "\n",
    "    with open(\"CSVs/ok_questions.csv\", \"w\", newline='', encoding=\"utf-8\") as out_f:\n",
    "        writer = csv.writer(out_f, quoting=csv.QUOTE_ALL)\n",
    "        for row in ok_rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "questions2 = pd.read_csv(\"CSVs/ok_questions.csv\", header=None)\n",
    "questions2 = questions2[[0]].values.tolist()\n",
    "\n",
    "def format_prompt2(qs):\n",
    "\n",
    "    header = f\"\"\"\n",
    "        You will classify each of the following questions into exactly one of the categories below.\n",
    "        Pick the **single best** category that fits the main intent of the question.\n",
    "\n",
    "        Categories:\n",
    "        - Author Bias\n",
    "        - Claim Verification\n",
    "        - Source Background & Credibility\n",
    "        - Publication Reputation\n",
    "        - Public Reaction\n",
    "\n",
    "        Return plain CSV only. Format:\n",
    "        \"<Question>\",\"<Category>\"\n",
    "\n",
    "        Example:\n",
    "        \"What is the background of journalist Jane Doe?\",\"Source Background & Credibility\"\n",
    "        \"\"\"\n",
    "\n",
    "    body = \"\\n\".join([f\"\\\"{q[0].strip()}\\\"\" for q in qs])\n",
    "    return header + body\n",
    "\n",
    "def process_batch2(index, chunk, retries=3):\n",
    "    prompt = format_prompt2(chunk)\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "                max_tokens=4096,\n",
    "            )\n",
    "            return index, response.choices[0].message.content.strip().splitlines()\n",
    "        except (OpenAIError, RateLimitError) as e:\n",
    "            wait = 2 ** attempt\n",
    "            time.sleep(wait)\n",
    "    return index, []\n",
    "batch_size = 50\n",
    "batches = [(i, questions2[i:i + batch_size]) for i in range(0, len(questions2), batch_size)]\n",
    "\n",
    "results = {}\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(process_batch2, i, chunk) for i, chunk in batches]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing\"):\n",
    "        idx, lines = future.result()\n",
    "        if lines:\n",
    "            results[idx] = lines\n",
    "        else:\n",
    "            print(f\"Batch {idx // batch_size} failed.\")\n",
    "\n",
    "# Sort results by original batch order\n",
    "ordered = []\n",
    "for i in sorted(results):\n",
    "    ordered.extend(results[i])\n",
    "\n",
    "print(f\"Total processed: {len(ordered)}\")\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"output1.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    # Filter out code block markers if present\n",
    "    filtered_lines = [line for line in ordered if line.strip() not in (\"```\", \"```plaintext\")]\n",
    "\n",
    "    for line in filtered_lines:\n",
    "        line = line.strip().strip('\"')\n",
    "        if not line:\n",
    "            continue\n",
    "        # Split on the last comma\n",
    "        if \",\" in line:\n",
    "            split_idx = line.rfind(\",\")\n",
    "            question = line[:split_idx].strip().strip('\"')\n",
    "            category = line[split_idx + 1:].strip().strip('\"')\n",
    "            writer.writerow([question, category])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
